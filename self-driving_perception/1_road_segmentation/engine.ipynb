{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "aef8a0fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, UpSampling2D, Dense, Reshape, Input, Add, Convolution2D, Flatten, Dropout, BatchNormalization, Conv2DTranspose, Activation, Lambda, Concatenate\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "from tensorflow.keras.applications import VGG16\n",
    "from tensorflow.keras.losses import MeanSquaredError, BinaryCrossentropy,SparseCategoricalCrossentropy\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras import callbacks\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from IPython.display import clear_output, display, HTML\n",
    "%matplotlib inline\n",
    "from base64 import b64encode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3e2d18",
   "metadata": {},
   "source": [
    "## Loading the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b6b01a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_dir = \"../data/one/training/image_2/\"\n",
    "train_gt_dir = \"../data/one/training/gt_image_2/\"\n",
    "test_data_dir = \"../data/one/testing/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "07ad8177",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 231\n",
      "Number of validation examples: 28\n",
      "Number of test examples: 30\n"
     ]
    }
   ],
   "source": [
    "# Number of training examples\n",
    "TRAINSET_SIZE = int(len(os.listdir(train_data_dir)) * 0.8)\n",
    "print(\"Number of training examples:\", TRAINSET_SIZE)\n",
    "\n",
    "VALIDSET_SIZE = int(len(os.listdir(train_data_dir)) * 0.1)\n",
    "print(\"Number of validation examples:\", VALIDSET_SIZE)\n",
    "\n",
    "TESTSET_SIZE = int(len(os.listdir(train_data_dir)) - TRAINSET_SIZE - VALIDSET_SIZE)\n",
    "print(\"Number of test examples:\", TESTSET_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3799fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize Constants\n",
    "IMG_SIZE = 128\n",
    "N_CHANNELS = 3\n",
    "N_CLASSES = 1\n",
    "SEED = 123"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "083c158a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to load images and return a dictionary\n",
    "def parse_image(image_path:str) -> dict:\n",
    "  image = tf.io.read_file(image_path)\n",
    "  image = tf.image.decode_jpeg(image, channels=N_CHANNELS)\n",
    "  image = tf.image.resize(image, [IMG_SIZE, IMG_SIZE])\n",
    "  \n",
    "  # The three types of img paths: um, umm, uu\n",
    "  # gt image paths: um_road, umm_read, uu_read\n",
    "  mask_path = tf.strings.regex_replace(image_path, \"image_2\", \"gt_image_2\")\n",
    "  mask_path = tf.strings.regex_replace(mask_path, \"um_\", \"um_road_\")\n",
    "  mask_path = tf.strings.regex_replace(mask_path, \"umm_\", \"umm_road_\")\n",
    "  \n",
    "  mask = tf.io.read_file(mask_path)\n",
    "  mask = tf.image.decode_jpeg(mask, channels=N_CHANNELS)\n",
    "  \n",
    "  non_road_label = np.array([255,0,0])\n",
    "  road_label = np.array([255,0,255])\n",
    "  other_label = np.array([0,0,0])\n",
    "  \n",
    "  # convert mask to binary mask\n",
    "  mask = tf.experimental.numpy.all(mask == road_label, axis=2)\n",
    "  mask = tf.cast(mask, tf.uint8)\n",
    "  mask = tf.expand_dims(mask, axis=-1)  # Add channel dimension\n",
    "  \n",
    "  \n",
    "  return {'image':image, 'segmentation_mask':mask}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "93f40d22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dataset variables\n",
    "all_dataset = tf.data.Dataset.list_files(train_data_dir + \"*.png\", seed=SEED)\n",
    "all_dataset = all_dataset.map(parse_image)\n",
    "\n",
    "train_dataset = all_dataset.take(TRAINSET_SIZE + VALIDSET_SIZE)\n",
    "val_dataset = train_dataset.skip(TRAINSET_SIZE)\n",
    "train_dataset = train_dataset.take(TRAINSET_SIZE)\n",
    "test_dataset = all_dataset.skip(TRAINSET_SIZE + VALIDSET_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "149fb73a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 231\n",
      "Number of validation examples: 28\n",
      "Number of test examples: 30\n"
     ]
    }
   ],
   "source": [
    "print(\"Number of training examples:\", len(train_dataset))\n",
    "print(\"Number of validation examples:\", len(val_dataset))\n",
    "print(\"Number of test examples:\", len(test_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "562215c9",
   "metadata": {},
   "source": [
    "## Applying transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9a641d4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tensorflow function to rescale images to [0, 1]\n",
    "@tf.function\n",
    "def normalize(input_image: tf.Tensor, input_mask: tf.Tensor) -> tuple:\n",
    "  input_image = tf.cast(input_image, tf.float32) / 255.0\n",
    "  return input_image, input_mask\n",
    "\n",
    "# Tensorflow function to apply preprocessing transformations\n",
    "@tf.function\n",
    "def load_image_train(datapoint: dict) -> tuple:\n",
    "  input_image = tf.image.resize(datapoint['image'], (IMG_SIZE, IMG_SIZE))\n",
    "  input_mask = tf.image.resize(datapoint['segmentation_mask'], (IMG_SIZE, IMG_SIZE))\n",
    "\n",
    "  if tf.random.uniform(()) > 0.5:\n",
    "    input_image = tf.image.flip_left_right(input_image)\n",
    "    input_mask = tf.image.flip_left_right(input_mask)\n",
    "\n",
    "  input_image, input_mask = normalize(input_image, input_mask)\n",
    "\n",
    "  return input_image, input_mask\n",
    "\n",
    "# Tensorflow function to preprocess validation images\n",
    "@tf.function\n",
    "def load_image_test(datapoint: dict) -> tuple:\n",
    "  input_image = tf.image.resize(datapoint['image'], (IMG_SIZE, IMG_SIZE))\n",
    "  input_mask = tf.image.resize(datapoint['segmentation_mask'], (IMG_SIZE, IMG_SIZE))\n",
    "\n",
    "  input_image, input_mask = normalize(input_image, input_mask)\n",
    "\n",
    "  return input_image, input_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cc78876a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<_PrefetchDataset element_spec=(TensorSpec(shape=(None, 128, 128, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 128, 128, 1), dtype=tf.float32, name=None))>\n",
      "<_PrefetchDataset element_spec=(TensorSpec(shape=(None, 128, 128, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 128, 128, 1), dtype=tf.float32, name=None))>\n",
      "<_PrefetchDataset element_spec=(TensorSpec(shape=(None, 128, 128, 3), dtype=tf.float32, name=None), TensorSpec(shape=(None, 128, 128, 1), dtype=tf.float32, name=None))>\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 32\n",
    "BUFFER_SIZE = 1000\n",
    "\n",
    "dataset = {\"train\": train_dataset, \"val\": val_dataset, \"test\": test_dataset}\n",
    "\n",
    "# -- Train Dataset --#\n",
    "dataset['train'] = dataset['train'].map(load_image_train, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "dataset['train'] = dataset['train'].shuffle(buffer_size=BUFFER_SIZE, seed=SEED)\n",
    "dataset['train'] = dataset['train'].repeat()\n",
    "dataset['train'] = dataset['train'].batch(BATCH_SIZE)\n",
    "dataset['train'] = dataset['train'].prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "#-- Validation Dataset --#\n",
    "dataset['val'] = dataset['val'].map(load_image_test)\n",
    "dataset['val'] = dataset['val'].repeat()\n",
    "dataset['val'] = dataset['val'].batch(BATCH_SIZE)\n",
    "dataset['val'] = dataset['val'].prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "#-- Testing Dataset --#\n",
    "dataset['test'] = dataset['test'].map(load_image_test)\n",
    "dataset['test'] = dataset['test'].batch(BATCH_SIZE)\n",
    "dataset['test'] = dataset['test'].prefetch(buffer_size=tf.data.AUTOTUNE)\n",
    "\n",
    "print(dataset['train'])\n",
    "print(dataset['val'])\n",
    "print(dataset['test'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6666ab01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
